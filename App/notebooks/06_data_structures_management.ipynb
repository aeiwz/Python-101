{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4f57ec",
   "metadata": {},
   "source": [
    "# 6. Data Structures & Data Management — Expanded + Use Cases (EN)\n",
    "\n",
    "This expansion adds:\n",
    "- **Collections** from the standard library (`Counter`, `defaultdict`, `deque`, `heapq`)\n",
    "- Robust **sorting**, **grouping**, and **joins** with lists/dicts\n",
    "- Practical **data management**: CSV/TSV/JSONL, `pathlib` batch I/O, **SQLite** mini‑demo\n",
    "- **Bioinformatics use cases** for genomics, proteomics, metabolomics, and microbiome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4d672",
   "metadata": {},
   "source": [
    "## 1) Collections you should know\n",
    "\n",
    "- `Counter`: frequency counts and top‑k\n",
    "- `defaultdict`: auto‑initialized containers (e.g., list of values per key)\n",
    "- `deque`: fast append/pop on both ends\n",
    "- `heapq`: priority queue / nlargest / nsmallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49af6f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter: Counter({'i': 4, 's': 4, 'p': 2, 'm': 1})\n",
      "most common 3: [('i', 4), ('s', 4), ('p', 2)]\n",
      "defaultdict groups: {'S1': [10, 9], 'S2': [12]}\n",
      "deque pop left: urgent | remaining: ['task1', 'task2']\n",
      "nlargest 3: [9, 8, 7]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict, deque\n",
    "import heapq\n",
    "\n",
    "# Counter\n",
    "counts = Counter(\"MISSISSIPPI\".lower())\n",
    "print(\"counter:\", counts)\n",
    "print(\"most common 3:\", counts.most_common(3))\n",
    "\n",
    "# defaultdict of lists\n",
    "groups = defaultdict(list)\n",
    "for name, score in [(\"S1\", 10), (\"S2\", 12), (\"S1\", 9)]:\n",
    "    groups[name].append(score)\n",
    "print(\"defaultdict groups:\", dict(groups))\n",
    "\n",
    "# deque as queue\n",
    "q = deque()\n",
    "q.append(\"task1\"); q.appendleft(\"urgent\"); q.append(\"task2\")\n",
    "print(\"deque pop left:\", q.popleft(), \"| remaining:\", list(q))\n",
    "\n",
    "# heapq\n",
    "arr = [5,1,9,3,7,2,8]\n",
    "print(\"nlargest 3:\", heapq.nlargest(3, arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabe4e3",
   "metadata": {},
   "source": [
    "## 2) Sorting tricks\n",
    "\n",
    "- Sort by a key: `sorted(items, key=lambda x: ...)`\n",
    "- Multi‑key sort with tuples: `key=lambda x: (x.city, -x.score)`\n",
    "- Stable sort: sort in passes (e.g., by secondary key first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750f9a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'S3', 'group': 'A', 'score': 92},\n",
       " {'id': 'S1', 'group': 'A', 'score': 88},\n",
       " {'id': 'S2', 'group': 'B', 'score': 92},\n",
       " {'id': 'S4', 'group': 'B', 'score': 75}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = [\n",
    "    {\"id\":\"S1\", \"group\":\"A\", \"score\": 88},\n",
    "    {\"id\":\"S2\", \"group\":\"B\", \"score\": 92},\n",
    "    {\"id\":\"S3\", \"group\":\"A\", \"score\": 92},\n",
    "    {\"id\":\"S4\", \"group\":\"B\", \"score\": 75},\n",
    "]\n",
    "\n",
    "by_group_score = sorted(records, key=lambda r: (r[\"group\"], -r[\"score\"], r[\"id\"]))\n",
    "by_group_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25abac",
   "metadata": {},
   "source": [
    "## 3) Grouping and \"joining\" in pure Python\n",
    "\n",
    "You can **group** rows into a dict of lists, and **join** two tables by building an index dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9943d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sample': 'S1', 'group': 'case', 'F1': 1.2, 'F2': 3.4},\n",
       " {'sample': 'S2', 'group': 'control', 'F1': 5.5, 'F2': 2.2},\n",
       " {'sample': 'S3', 'group': 'case', 'F1': 0.7, 'F2': 1.1}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Two \"tables\"\n",
    "samples = [\n",
    "    {\"sample\":\"S1\", \"group\":\"case\"},\n",
    "    {\"sample\":\"S2\", \"group\":\"control\"},\n",
    "    {\"sample\":\"S3\", \"group\":\"case\"},\n",
    "]\n",
    "measure = [\n",
    "    {\"sample\":\"S1\", \"F1\": 1.2, \"F2\": 3.4},\n",
    "    {\"sample\":\"S3\", \"F1\": 0.7, \"F2\": 1.1},\n",
    "    {\"sample\":\"S2\", \"F1\": 5.5, \"F2\": 2.2},\n",
    "]\n",
    "\n",
    "# Build an index by sample for fast join (hash join)\n",
    "idx = {row[\"sample\"]: row for row in measure}\n",
    "\n",
    "joined = []\n",
    "for meta in samples:\n",
    "    m = idx.get(meta[\"sample\"], {})\n",
    "    joined.append({**meta, **m})  # merge dicts\n",
    "joined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced21ff",
   "metadata": {},
   "source": [
    "## 4) Data management formats: CSV, TSV, JSONL, batch I/O\n",
    "\n",
    "- **CSV/TSV**: rectangular, widely supported\n",
    "- **JSONL** (JSON lines): one JSON object per line (easy streaming)\n",
    "- Use `pathlib.Path.glob()` to batch over files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e4edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample,reads', 'S1,1455']\n",
      "['sample\\treads', 'S1\\t1455']\n",
      "JSONL first line: {\"sample\": \"S1\", \"reads\": 1455}\n"
     ]
    }
   ],
   "source": [
    "import csv, json, pathlib, random\n",
    "\n",
    "# Create CSV and TSV\n",
    "rows = [{\"sample\":f\"S{i}\", \"reads\": random.randint(800, 1500)} for i in range(1,5)]\n",
    "csv_path = pathlib.Path(\"reads.csv\")\n",
    "tsv_path = pathlib.Path(\"reads.tsv\")\n",
    "\n",
    "with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"sample\",\"reads\"])\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "\n",
    "with tsv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"sample\",\"reads\"], delimiter=\"\\t\")\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "\n",
    "# JSONL\n",
    "jsonl_path = pathlib.Path(\"reads.jsonl\")\n",
    "with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "print(csv_path.read_text().strip().splitlines()[:2])\n",
    "print(tsv_path.read_text().strip().splitlines()[:2])\n",
    "print(\"JSONL first line:\", jsonl_path.read_text().splitlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8b342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  tiny.csv (no 'reads' column)\n",
      "✅ reads.csv enc=utf-8-sig +5117\n",
      "————————————————————————————————————————\n",
      "Total reads across CSV files: 5117\n",
      "Wrote reads_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import csv, pathlib, sys, importlib\n",
    "\n",
    "CSV_GLOB = \"**/*.csv\"          # recurse subfolders; use \"*.csv\" for current folder only\n",
    "READS_KEYS = {\"reads\",\"read\",\"read_count\",\"readcount\",\"num_reads\",\"total_reads\"}\n",
    "\n",
    "def open_text_with_fallback(path: pathlib.Path):\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"latin-1\", \"cp1252\"):\n",
    "        try:\n",
    "            f = path.open(\"r\", encoding=enc, newline=\"\")\n",
    "            # force an initial decode to surface errors now\n",
    "            _ = f.read(0); f.seek(0)\n",
    "            return f, enc\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise UnicodeDecodeError(\"all\", b\"\", 0, 1, \"no suitable encoding\")\n",
    "\n",
    "def norm(name: str) -> str:\n",
    "    return name.strip().lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "def parse_int_safe(val):\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        raise ValueError(\"empty\")\n",
    "    return int(s.replace(\",\", \"\").replace(\" \", \"\"))\n",
    "\n",
    "def find_reads_key(fieldnames):\n",
    "    if not fieldnames:\n",
    "        return None\n",
    "    mapping = {norm(h): h for h in fieldnames}\n",
    "    for k in READS_KEYS:\n",
    "        if k in mapping:\n",
    "            return mapping[k]\n",
    "    for k, orig in mapping.items():\n",
    "        if \"read\" in k and \"percent\" not in k:\n",
    "            return orig\n",
    "    return None\n",
    "\n",
    "total = 0\n",
    "summary = []\n",
    "\n",
    "for p in pathlib.Path(\".\").rglob(\"*.csv\"):   # or .glob(CSV_GLOB)\n",
    "    # skip macOS AppleDouble and hidden dotfiles\n",
    "    if p.name.startswith(\"._\") or p.name.startswith(\".\"):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        f, enc = open_text_with_fallback(p)\n",
    "    except UnicodeDecodeError:\n",
    "        summary.append({\"file\": p.as_posix(), \"status\": \"encoding_error\", \"enc\": None, \"sum\": 0, \"bad_rows\": None})\n",
    "        continue\n",
    "\n",
    "    with f:\n",
    "        try:\n",
    "            r = csv.DictReader(f)\n",
    "            col = find_reads_key(r.fieldnames)\n",
    "            if not col:\n",
    "                summary.append({\"file\": p.as_posix(), \"status\": \"no_reads_col\", \"enc\": enc, \"sum\": 0, \"bad_rows\": None})\n",
    "                continue\n",
    "\n",
    "            file_sum, bad_rows = 0, 0\n",
    "            for row in r:\n",
    "                try:\n",
    "                    file_sum += parse_int_safe(row.get(col))\n",
    "                except Exception:\n",
    "                    bad_rows += 1\n",
    "            total += file_sum\n",
    "            summary.append({\"file\": p.as_posix(), \"status\": \"ok\", \"enc\": enc, \"sum\": file_sum, \"bad_rows\": bad_rows})\n",
    "        except Exception as e:\n",
    "            summary.append({\"file\": p.as_posix(), \"status\": f\"parse_error:{type(e).__name__}\", \"enc\": enc, \"sum\": 0, \"bad_rows\": None})\n",
    "\n",
    "# print a neat report\n",
    "for s in summary:\n",
    "    if s[\"status\"] == \"ok\":\n",
    "        extra = f\" (skipped {s['bad_rows']} bad rows)\" if s[\"bad_rows\"] else \"\"\n",
    "        print(f\" {s['file']} enc={s['enc']} +{s['sum']}{extra}\")\n",
    "    elif s[\"status\"] == \"no_reads_col\":\n",
    "        print(f\"  {s['file']} (no 'reads' column)\")\n",
    "    elif s[\"status\"] == \"encoding_error\":\n",
    "        print(f\" {s['file']} (encoding not recognized)\")\n",
    "    else:\n",
    "        print(f\" {s['file']} ({s['status']})\")\n",
    "\n",
    "print(\"—\" * 40)\n",
    "print(\"Total reads across CSV files:\", total)\n",
    "\n",
    "# (Optional) write a summary CSV\n",
    "with open(\"reads_summary.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as out:\n",
    "    w = csv.DictWriter(out, fieldnames=[\"file\",\"status\",\"enc\",\"sum\",\"bad_rows\"])\n",
    "    w.writeheader(); w.writerows(summary)\n",
    "print(\"Wrote reads_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470623ad",
   "metadata": {},
   "source": [
    "## 5) Mini SQL database with `sqlite3` (stdlib)\n",
    "\n",
    "- Create a table, insert rows, run queries with filtering & aggregation.\n",
    "- SQLite is a file‑based database—great for small/medium datasets and teaching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "001c0432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg reads: [('case', 1350.0), ('control', 900.0)]\n",
      "top sample: ('S3', 1500)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3, pathlib, os\n",
    "\n",
    "db = \"lab.db\"\n",
    "if os.path.exists(db):\n",
    "    os.remove(db)\n",
    "con = sqlite3.connect(db)\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\"CREATE TABLE sample (id TEXT PRIMARY KEY, group_name TEXT, reads INT)\")\n",
    "cur.executemany(\"INSERT INTO sample VALUES (?,?,?)\", [\n",
    "    (\"S1\",\"case\", 1200),\n",
    "    (\"S2\",\"control\", 900),\n",
    "    (\"S3\",\"case\", 1500),\n",
    "])\n",
    "\n",
    "# Query: average reads per group\n",
    "cur.execute(\"SELECT group_name, AVG(reads) FROM sample GROUP BY group_name\")\n",
    "print(\"avg reads:\", cur.fetchall())\n",
    "\n",
    "# Query: top sample by reads\n",
    "cur.execute(\"SELECT id, reads FROM sample ORDER BY reads DESC LIMIT 1\")\n",
    "print(\"top sample:\", cur.fetchone())\n",
    "\n",
    "con.commit(); con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217d4c59",
   "metadata": {},
   "source": [
    "## 6) Bioinformatics use cases\n",
    "\n",
    "### (a) Genomics — FASTA index (length, GC)\n",
    "Build a dictionary index from FASTA (pure-Python reader), then compute summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4936ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq1': {'len': 10, 'gc': 0.5}, 'seq2': {'len': 10, 'gc': 0.4}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tiny FASTA and pure-python reader\n",
    "fasta = \">seq1\\nATGCGTACGT\\n>seq2\\nATTTGGCCAA\\n\"\n",
    "open(\"tiny.fasta\", \"w\").write(fasta)\n",
    "\n",
    "def read_fasta(path):\n",
    "    name, seq = None, []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('>'):\n",
    "                if name:\n",
    "                    yield (name, ''.join(seq))\n",
    "                name, seq = line[1:], []\n",
    "            else:\n",
    "                seq.append(line)\n",
    "    if name:\n",
    "        yield (name, ''.join(seq))\n",
    "\n",
    "index = {}\n",
    "for name, seq in read_fasta(\"tiny.fasta\"):\n",
    "    gc = (seq.count(\"G\")+seq.count(\"C\"))/len(seq)\n",
    "    index[name] = {\"len\": len(seq), \"gc\": gc}\n",
    "\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb6c65",
   "metadata": {},
   "source": [
    "### (b) Proteomics — top‑k peptides by mass using a heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "735821aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PEPTIDE', '799.35994'), ('MKW', '463.22532'), ('WY', '367.15320')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "import heapq\n",
    "\n",
    "AA_MASS = {\n",
    "    \"A\": Decimal(\"71.03711\"),  \"C\": Decimal(\"103.00919\"),\n",
    "    \"D\": Decimal(\"115.02694\"), \"E\": Decimal(\"129.04259\"),\n",
    "    \"F\": Decimal(\"147.06841\"), \"G\": Decimal(\"57.02146\"),\n",
    "    \"H\": Decimal(\"137.05891\"), \"I\": Decimal(\"113.08406\"),\n",
    "    \"K\": Decimal(\"128.09496\"), \"L\": Decimal(\"113.08406\"),\n",
    "    \"M\": Decimal(\"131.04049\"), \"N\": Decimal(\"114.04293\"),\n",
    "    \"P\": Decimal(\"97.05276\"),  \"Q\": Decimal(\"128.05858\"),\n",
    "    \"R\": Decimal(\"156.10111\"), \"S\": Decimal(\"87.03203\"),\n",
    "    \"T\": Decimal(\"101.04768\"), \"V\": Decimal(\"99.06841\"),\n",
    "    \"W\": Decimal(\"186.07931\"), \"Y\": Decimal(\"163.06333\"),\n",
    "}\n",
    "WATER = Decimal(\"18.01056\")\n",
    "\n",
    "def pep_mass(pep: str) -> Decimal:\n",
    "    m = WATER\n",
    "    for aa in pep:\n",
    "        m += AA_MASS[aa]\n",
    "    return m\n",
    "\n",
    "peptides = [\"ACD\", \"MKW\", \"PEPTIDE\", \"AAAA\", \"WY\"]\n",
    "top3 = heapq.nlargest(3, peptides, key=pep_mass)\n",
    "[(p, str(pep_mass(p))) for p in top3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e0753d",
   "metadata": {},
   "source": [
    "### (c) Metabolomics — assemble a data matrix from multiple CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "435ea66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'F1', 'F2', 'F3', 'F4']\n",
      "['S1', 80.77, 96.99, 43.13, 16.49]\n",
      "['S2', 95.41, 22.4, 20.66, 62.79]\n",
      "['S3', 57.97, 30.86, 60.35, 22.94]\n"
     ]
    }
   ],
   "source": [
    "# Create 3 small CSV files with intensities for the same features\n",
    "import csv, pathlib, random\n",
    "features = [\"F1\",\"F2\",\"F3\",\"F4\"]\n",
    "for sid in [\"S1\",\"S2\",\"S3\"]:\n",
    "    with open(f\"{sid}.csv\",\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f); w.writerow([\"feature\",\"intensity\"])\n",
    "        for feat in features:\n",
    "            w.writerow([feat, round(random.uniform(0, 100), 2)])\n",
    "\n",
    "# Build matrix: rows=samples, cols=features\n",
    "samples = [\"S1\",\"S2\",\"S3\"]\n",
    "matrix = {s:{} for s in samples}\n",
    "for s in samples:\n",
    "    with open(f\"{s}.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            matrix[s][row[\"feature\"]] = float(row[\"intensity\"])\n",
    "\n",
    "# Make a consistent table (fill missing with 0.0)\n",
    "ordered = sorted(features)\n",
    "table = [[\"sample\", *ordered]]\n",
    "for s in samples:\n",
    "    table.append([s, *[matrix[s].get(feat, 0.0) for feat in ordered]])\n",
    "\n",
    "for row in table:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726656b6",
   "metadata": {},
   "source": [
    "### (d) Microbiome — group by taxonomic rank with `defaultdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29098e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Escherichia': 0.714, 'Bacteroides': 0.286}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Suppose each OTU has an associated genus\n",
    "otu_rows = [\n",
    "    {\"otu\":\"OTU1\", \"genus\":\"Escherichia\", \"count\": 1200},\n",
    "    {\"otu\":\"OTU2\", \"genus\":\"Bacteroides\", \"count\": 600},\n",
    "    {\"otu\":\"OTU3\", \"genus\":\"Escherichia\", \"count\": 300},\n",
    "]\n",
    "\n",
    "by_genus = defaultdict(int)\n",
    "for row in otu_rows:\n",
    "    by_genus[row[\"genus\"]] += row[\"count\"]\n",
    "\n",
    "total = sum(by_genus.values()) or 1\n",
    "ra = {g: c/total for g, c in by_genus.items()}\n",
    "{g: round(v,3) for g,v in ra.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a26d979",
   "metadata": {},
   "source": [
    "## 7) Exercises\n",
    "\n",
    "1. **Top‑k GC FASTA**: Extend the FASTA index example to print the **top‑2 sequences by GC%** using `heapq.nlargest`.\n",
    "2. **Join + Group**: Join `samples` and `measure` above, then compute the **mean F1** per group.\n",
    "3. **SQLite filter**: In the SQLite demo, add a query that returns **all case samples with reads > 1000**.\n",
    "4. **Metabolomics merge**: Extend the matrix builder to **normalize each row** so its sum is 1.0 and recompute the **max feature** per sample.\n",
    "5. **Microbiome rarity**: Using `by_genus`, return a list of genera with relative abundance **< 5%**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
